# -*- coding: utf-8 -*-
"""Hackathon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PEXLxRQMn5gp4yUxgioz3rcOKu-FJ9rb
"""

import pandas as pd
import numpy as np
import datetime
import keras

test_data= pd.read_csv("/content/test_student.csv")
dataset = pd.read_csv("/content/train.csv")
dataset.dtypes

def time_second_conv(time):
        # time must be a datetime object
        h = time.hour
        m = time.minute
        s = time.second
        return s + m*60 + h*3600

def check_nan(df):
        columns_name = df.columns
        nan = pd.isna(df)
        for col in columns_name:
            print(nan[col].value_counts())        

def preprocess(df):
    
    # drop useless columns
    col_to_drop = ["from", "to", "Id", "date", "index"]
    df.drop(columns=col_to_drop, inplace=True)
    
    """====================check NaN values======================="""
    
    check_nan(df)
        
    """====================split labels======================="""
    
    labels = df.pop("delay_minutes")
    labels = pd.DataFrame(labels)
    
    """====================manage schedule_time======================="""
    
    # create year, month, day, hour columns
    df['scheduled_time'] = pd.to_datetime(df['scheduled_time'])

    df['scheduled_hour'] = [time_second_conv(d.time()) for d in df['scheduled_time']]
    df['scheduled_day'] = df['scheduled_time'].dt.day
    df['scheduled_month'] = df['scheduled_time'].dt.month
    df['scheduled_year'] = df['scheduled_time'].dt.year
    
    # drop scheduled_time
    df.drop(columns=["scheduled_time"], inplace=True)
    
    """======================normalization and categorization====================="""
    
    df = pd.concat([df.drop("line", axis=1), pd.get_dummies(df["line"])], axis=1)
    df = pd.concat([df.drop("status", axis=1), pd.get_dummies(df["status"])], axis=1)
    df = pd.concat([df.drop("from_id", axis=1), pd.get_dummies(df["from_id"], prefix='from')], axis=1)
    df = pd.concat([df.drop("to_id", axis=1), pd.get_dummies(df["to_id"],prefix='to')], axis=1)
    df = pd.concat([df.drop("train_id", axis=1), pd.get_dummies(df["train_id"],prefix='train')], axis=1)
    
    # normalize hours
    normalize_hour = lambda x: x/(24*3600) #x has to be in second over 24 hours
    df['scheduled_hour'] = df['scheduled_hour'].apply(normalize_hour)

    return df, labels

def preprocess_test(df):
    
    # drop useless columns
    col_to_drop = ["from", "to", "Id", "date", "index"]
    df.drop(columns=col_to_drop, inplace=True)
    
    """====================check NaN values======================="""
    
    check_nan(df)
    
    """====================manage schedule_time======================="""
    
    # create year, month, day, hour columns
    df['scheduled_time'] = pd.to_datetime(df['scheduled_time'])

    df['scheduled_hour'] = [time_second_conv(d.time()) for d in df['scheduled_time']]
    df['scheduled_day'] = df['scheduled_time'].dt.day
    df['scheduled_month'] = df['scheduled_time'].dt.month
    df['scheduled_year'] = df['scheduled_time'].dt.year
    
    # drop scheduled_time
    df.drop(columns=["scheduled_time"], inplace=True)
    
    """======================normalization and categorization====================="""
    
    df = pd.concat([df.drop("line", axis=1), pd.get_dummies(df["line"])], axis=1)
    df = pd.concat([df.drop("status", axis=1), pd.get_dummies(df["status"])], axis=1)
    df = pd.concat([df.drop("from_id", axis=1), pd.get_dummies(df["from_id"], prefix='from')], axis=1)
    df = pd.concat([df.drop("to_id", axis=1), pd.get_dummies(df["to_id"],prefix='to')], axis=1)
    df = pd.concat([df.drop("train_id", axis=1), pd.get_dummies(df["train_id"],prefix='train')], axis=1)
    
    # normalize hours
    normalize_hour = lambda x: x/(24*3600) #x has to be in second over 24 hours
    df['scheduled_hour'] = df['scheduled_hour'].apply(normalize_hour)
    
    return df

dataset, labels = preprocess(dataset)
test_data = preprocess_test(test_data)

# Normalization by global max(stop_sequence)
maxval = max(dataset['stop_sequence'])
normalize_maxwise = lambda x: x/maxval
dataset["stop_sequence"] = dataset["stop_sequence"].apply(normalize_maxwise)

# Normalization by percentage of sequence
def normalization_percentagewise(start, end, m, df):
  df.loc[start:end, 'stop_sequence'] = df.loc[start:end, 'stop_sequence'].apply(lambda x: x/m)

def normalize_df_percentagewise(df) :
  start = 0
  end = 0
  for i in range(len(df['stop_sequence'])-1) :
    m = df.at[i, 'stop_sequence']
    if df.at[i+1, 'stop_sequence'] <= m :
        end +=1   # Pour la bonne dimension de range()
        normalization_percentagewise(start, end, m, df)
        start = i+1
        end = i+1
    else :
      end += 1

  normalization_percentagewise(start, end, df.at[len(df['stop_sequence'])-1, 'stop_sequence'], df)

normalize_df_percentagewise(dataset)
normalize_df_percentagewise(test_data)

# Normalization by max(stop_sequence) per train
data1 = pd.read_csv("/content/train.csv")
test1 = pd.read_csv("/content/test_student.csv")

def normalize_trainwise(df, List) :
  for element in List :
    col = "train_" + str(element)
    mask = (df[col] == 1)
    d_list = df.loc[mask]
    max_list = max(d_list['stop_sequence'])
    df.loc[mask, 'stop_sequence'] = d_list['stop_sequence'].apply(lambda x: x/max_list)

Train_d = data1.train_id.unique()
Train_t = test1.train_id.unique()
normalize_linewise(dataset, Train_d)
normalize_linewise(test_data, Train_t)

# Normalization by max(stop_sequence) per line
data2 = pd.read_csv("/content/train.csv")
test2 = pd.read_csv("/content/test_student.csv")
def normalize_linewise(df, List) :
  for element in List :
    mask = (df[element] == 1)
    d_list = df.loc[mask]
    max_list = max(d_list['stop_sequence'])
    df.loc[mask, 'stop_sequence'] = d_list['stop_sequence'].apply(lambda x: x/max_list)

Line_d = data2.line.unique()
Line_t = test2.line.unique()
normalize_linewise(dataset, Line_d)
normalize_linewise(test_data, Line_t)

dataset.head()

test_data.head()

# Get missing columns in the training set
missing_cols = set( dataset.columns ) - set( test_data.columns )
for c in missing_cols:
    dataset.drop(columns=c, inplace=True)

# Get missing columns in the testing set
missing_cols_inv = set( test_data.columns ) - set( dataset.columns )
for c in missing_cols_inv:
    test_data.drop(columns=c, inplace=True)

print(dataset.shape)
print(test_data.shape)

dataset.head()

test_data.head()

def build_model():
  model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=[724]),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(1)
  ])

  from sklearn.metrics import mean_squared_error

  model.compile(loss= "mean_squared_error" , optimizer="adam", metrics=["mean_squared_error"])
  return model


model = build_model()

model.fit(dataset, labels, epochs=25)